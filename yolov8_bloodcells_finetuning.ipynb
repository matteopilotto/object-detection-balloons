{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "import ast\n",
    "import pybboxes as pbx\n",
    "\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import wandb\n",
    "\n",
    "import random\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"object-detection-yolov8\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"yolov8_bloodcells_finetuning.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è 'ultralytics.yolo.v8' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.models.yolo' instead.\n",
      "WARNING ‚ö†Ô∏è 'ultralytics.yolo.utils' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.utils' instead.\n",
      "Note this warning may be related to loading older models. You can update your model to current structure with:\n",
      "    import torch\n",
      "    ckpt = torch.load(\"model.pt\")  # applies to both official and custom models\n",
      "    torch.save(ckpt, \"updated-model.pt\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ultralytics.yolo.utils.torch_utils import get_flops, get_num_params\n",
    "\n",
    "# try:\n",
    "#     import wandb\n",
    "\n",
    "#     assert hasattr(wandb, '__version__')\n",
    "# except (ImportError, AssertionError):\n",
    "#     wandb = None\n",
    "\n",
    "\n",
    "def on_pretrain_routine_start(trainer):\n",
    "    wandb.init(\n",
    "        project=trainer.args.project or \"YOLOv8\", \n",
    "        name=trainer.args.name, \n",
    "        save_code=True,\n",
    "        config=dict(trainer.args)\n",
    "    ) if not wandb.run else wandb.run\n",
    "\n",
    "\n",
    "def on_fit_epoch_end(trainer):\n",
    "    wandb.run.log(trainer.metrics, step=trainer.epoch + 1)\n",
    "\n",
    "    if trainer.epoch == 0:\n",
    "        model_info = {\n",
    "            \"model/parameters\": get_num_params(trainer.model),\n",
    "            \"model/GFLOPs\": round(get_flops(trainer.model), 3),\n",
    "            \"model/speed(ms)\": round(sum(trainer.validator.speed.values()), 3)\n",
    "        }\n",
    "        wandb.run.log(model_info, step=trainer.epoch + 1)\n",
    "\n",
    "\n",
    "def on_train_epoch_end(trainer):\n",
    "    wandb.run.log(trainer.label_loss_items(trainer.tloss, prefix=\"train\"), step=trainer.epoch + 1)\n",
    "    wandb.run.log(trainer.lr, step=trainer.epoch + 1)\n",
    "    print(trainer.save_dir)\n",
    "    if (trainer.epoch + 1) % 1 == 0:\n",
    "        # wandb.run.log({f.stem: wandb.Image(str(f)) for f in trainer.save_dir.glob('train_batch*.jpg')}, step=trainer.epoch + 1)\n",
    "        images = [wandb.Image(str(img_path), caption=img_path.stem) for img_path in Path(trainer.save_dir).glob(\"*.jpg\")]\n",
    "        wandb.run.log({f\"train set | {trainer.args.name} | train\": images}, step=trainer.epoch + 1)\n",
    "\n",
    "\n",
    "def on_train_end(trainer):\n",
    "    art = wandb.Artifact(type=\"model\", name=f\"run_{wandb.run.id}_model\")\n",
    "    if trainer.best.exists():\n",
    "        art.add_file(trainer.best)\n",
    "        wandb.run.log_artifact(art)\n",
    "    \n",
    "    # wandb.run.finish()\n",
    "\n",
    "\n",
    "def on_predict_start(predictor):\n",
    "    wandb.init(\n",
    "        project=os.getenv(\"WANDB_PROJECT\"), \n",
    "        # name=predictor.args.name,\n",
    "        job_type=\"prediction\",\n",
    "        tags=[\"prediction\", predictor.args.name],\n",
    "        save_code=True,\n",
    "        config=dict(predictor.args)\n",
    "    ) if not wandb.run else wandb.run\n",
    "\n",
    "def on_predict_end(predictor):\n",
    "    split = Path(predictor.data_path).parent.stem\n",
    "    images = [wandb.Image(str(img_path), caption=img_path.stem) for img_path in Path(predictor.save_dir).glob(\"*.jpg\")]\n",
    "    wandb.run.log({f\"{split} set | {predictor.args.name} | pred\": images})\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = {\n",
    "    \"on_pretrain_routine_start\": on_pretrain_routine_start,\n",
    "    \"on_train_epoch_end\": on_train_epoch_end,\n",
    "    \"on_fit_epoch_end\": on_fit_epoch_end,\n",
    "    \"on_train_end\": on_train_end,\n",
    "    \"on_predict_start\": on_predict_start,\n",
    "    \"on_predict_end\": on_predict_end,\n",
    "} if wandb else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"keremberke/blood-cell-object-detection\", \"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/workspace/object-detection-balloons/datasets/bloodcells\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm -rf /workspace/object-detection-balloons/datasets/bloodcells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in dataset:\n",
    "    for row in dataset[split]:\n",
    "        img_path = Path(os.path.join(dataset_dir, \"images\", split, str(row[\"image_id\"])) + \".jpg\")\n",
    "        os.makedirs(img_path.parent, exist_ok=True)\n",
    "        row[\"image\"].save(img_path)\n",
    "        for bbox, category in zip(row[\"objects\"][\"bbox\"], row[\"objects\"][\"category\"]):\n",
    "            bbox_yolo = pbx.convert_bbox(bbox, from_type=\"coco\", to_type=\"yolo\", image_size=(row[\"width\"], row[\"height\"]))\n",
    "            bbox_yolo = \" \".join([str(bb) for bb in bbox_yolo])\n",
    "            file_path = Path(os.path.join(dataset_dir, \"labels\", split, str(row[\"image_id\"])) + \".txt\")\n",
    "            os.makedirs(file_path.parent, exist_ok=True)\n",
    "            with open(file_path, \"a\") as f:\n",
    "                f.write(f\"{category} {bbox_yolo}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.170 üöÄ Python-3.10.6 torch-2.0.1+cu118 CUDA:0 (Tesla V100-FHHL-16GB, 16151MiB)\n",
      "Setup complete ‚úÖ (32 CPUs, 94.3 GB RAM, 1.3/20.0 GB disk)\n"
     ]
    }
   ],
   "source": [
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/workspace/object-detection-balloons/datasets/bloodcells/images/validation/51.jpg', '/workspace/object-detection-balloons/datasets/bloodcells/images/validation/14.jpg', '/workspace/object-detection-balloons/datasets/bloodcells/images/validation/48.jpg']\n",
      "['/workspace/object-detection-balloons/datasets/bloodcells/images/test/10.jpg', '/workspace/object-detection-balloons/datasets/bloodcells/images/test/32.jpg', '/workspace/object-detection-balloons/datasets/bloodcells/images/test/34.jpg']\n"
     ]
    }
   ],
   "source": [
    "val_img_base_path = \"/workspace/object-detection-balloons/datasets/bloodcells/images/validation\"\n",
    "val_img_paths = [os.path.join(val_img_base_path, fname) for fname in os.listdir(val_img_base_path)]\n",
    "print(val_img_paths[:3])\n",
    "\n",
    "test_img_base_path = \"/workspace/object-detection-balloons/datasets/bloodcells/images/test\"\n",
    "test_img_paths = [os.path.join(test_img_base_path, fname) for fname in os.listdir(test_img_base_path)]\n",
    "print(test_img_paths[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in dataset:\n",
    "    for row in dataset[split]:\n",
    "        img_path = Path(os.path.join(dataset_dir, \"images\", split, str(row[\"image_id\"])) + \".jpg\")\n",
    "        os.makedirs(img_path.parent, exist_ok=True)\n",
    "        row[\"image\"].save(img_path)\n",
    "        for bbox, category in zip(row[\"objects\"][\"bbox\"], row[\"objects\"][\"category\"]):\n",
    "            bbox_yolo = pbx.convert_bbox(bbox, from_type=\"coco\", to_type=\"yolo\", image_size=(row[\"width\"], row[\"height\"]))\n",
    "            bbox_yolo = \" \".join([str(bb) for bb in bbox_yolo])\n",
    "            file_path = Path(os.path.join(dataset_dir, \"labels\", split, str(row[\"image_id\"])) + \".txt\")\n",
    "            os.makedirs(file_path.parent, exist_ok=True)\n",
    "            with open(file_path, \"a\") as f:\n",
    "                f.write(f\"{category} {bbox_yolo}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log images on WANDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/object-detection-balloons/wandb/run-20230904_121039-z16cvqka</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matt24/object-detection-yolov8/runs/z16cvqka' target=\"_blank\">solar-haze-22</a></strong> to <a href='https://wandb.ai/matt24/object-detection-yolov8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matt24/object-detection-yolov8' target=\"_blank\">https://wandb.ai/matt24/object-detection-yolov8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matt24/object-detection-yolov8/runs/z16cvqka' target=\"_blank\">https://wandb.ai/matt24/object-detection-yolov8/runs/z16cvqka</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbd09451d7f49bfb0481721cd4be4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Only 108 Image will be uploaded.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (SSLError), entering retry loop.\n",
      "wandb: Network error (SSLError), entering retry loop.\n",
      "wandb: Network error (SSLError), entering retry loop.\n",
      "wandb: Network error (SSLError), entering retry loop.\n",
      "wandb: Network error (SSLError), entering retry loop.\n",
      "wandb: Network error (SSLError), entering retry loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-haze-22</strong> at: <a href='https://wandb.ai/matt24/object-detection-yolov8/runs/z16cvqka' target=\"_blank\">https://wandb.ai/matt24/object-detection-yolov8/runs/z16cvqka</a><br/> View job at <a href='https://wandb.ai/matt24/object-detection-yolov8/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk0ODQyMTk4/version_details/v7' target=\"_blank\">https://wandb.ai/matt24/object-detection-yolov8/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk0ODQyMTk4/version_details/v7</a><br/>Synced 5 W&B file(s), 434 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230904_121039-z16cvqka/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(job_type=\"dataset\")\n",
    "\n",
    "class_id_to_label = {\n",
    "    0: \"platelets\",\n",
    "    1: \"rbc\",\n",
    "    2: \"wbc\",\n",
    "}\n",
    "\n",
    "images = []\n",
    "for split in tqdm(dataset):\n",
    "    for row in dataset[split]:\n",
    "        positions = [\n",
    "            dict(zip((\"minX\", \"minY\", \"maxX\", \"maxY\"), pbx.convert_bbox(bbox, from_type=\"coco\", to_type=\"voc\", image_size=(row[\"width\"], row[\"height\"]))))\n",
    "            for bbox in row[\"objects\"][\"bbox\"]\n",
    "        ]\n",
    "\n",
    "        class_ids = row[\"objects\"][\"category\"]\n",
    "        box_captions = [class_id_to_label[id] for id in class_ids]\n",
    "\n",
    "        box_data = [dict(zip((\"position\", \"class_id\", \"box_caption\", \"domain\"), x)) for x in zip(positions, class_ids, box_captions, [\"pixel\"] * len(class_ids))]\n",
    "\n",
    "        images.append(wandb.Image(row[\"image\"], caption=str(row[\"image_id\"]), boxes={\"ground_truth\": {\"box_data\": box_data}}))\n",
    "\n",
    "    # run.finish()\n",
    "    # break\n",
    "    run.log({f\"{split}_set\": images})\n",
    "    images = []\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'yolov8n.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/object-detection-balloons/wandb/run-20230904_140501-pnxx1nod</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matt24/object-detection-yolov8/runs/pnxx1nod' target=\"_blank\">fiery-disco-34</a></strong> to <a href='https://wandb.ai/matt24/object-detection-yolov8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matt24/object-detection-yolov8' target=\"_blank\">https://wandb.ai/matt24/object-detection-yolov8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matt24/object-detection-yolov8/runs/pnxx1nod' target=\"_blank\">https://wandb.ai/matt24/object-detection-yolov8/runs/pnxx1nod</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 640x640 1 toothbrush, 1: 640x640 (no detections), 2: 640x640 1 person, 3: 640x640 1 teddy bear, 4: 640x640 1 person, 5: 640x640 (no detections), 6: 640x640 2 persons, 7: 640x640 (no detections), 8: 640x640 (no detections), 9: 640x640 (no detections), 10: 640x640 1 person, 11: 640x640 1 person, 12: 640x640 2 persons, 13: 640x640 (no detections), 14: 640x640 (no detections), 15: 640x640 1 person, 16: 640x640 (no detections), 17: 640x640 (no detections), 18: 640x640 1 person, 1 sports ball, 19: 640x640 1 person, 20: 640x640 1 toothbrush, 21: 640x640 1 person, 22: 640x640 (no detections), 23: 640x640 1 toothbrush, 24: 640x640 1 person, 1 toothbrush, 25: 640x640 1 donut, 26: 640x640 2 persons, 27: 640x640 (no detections), 28: 640x640 (no detections), 29: 640x640 (no detections), 30: 640x640 2 sports balls, 1 toothbrush, 31: 640x640 1 donut, 32: 640x640 1 sports ball, 33: 640x640 1 person, 1 sports ball, 34: 640x640 (no detections), 35: 640x640 (no detections), 36: 640x640 (no detections), 37: 640x640 1 donut, 38: 640x640 1 cake, 39: 640x640 1 donut, 40: 640x640 (no detections), 41: 640x640 (no detections), 42: 640x640 1 donut, 43: 640x640 (no detections), 44: 640x640 1 toothbrush, 45: 640x640 (no detections), 46: 640x640 (no detections), 47: 640x640 (no detections), 48: 640x640 1 person, 1 sports ball, 49: 640x640 1 sports ball, 50: 640x640 2 sports balls, 51: 640x640 1 sports ball, 52: 640x640 (no detections), 53: 640x640 1 donut, 1 toothbrush, 54: 640x640 1 toothbrush, 55: 640x640 (no detections), 56: 640x640 2 toothbrushs, 57: 640x640 1 sports ball, 58: 640x640 (no detections), 59: 640x640 (no detections), 60: 640x640 1 toothbrush, 61: 640x640 2 sports balls, 62: 640x640 1 toothbrush, 63: 640x640 1 toothbrush, 64: 640x640 (no detections), 65: 640x640 (no detections), 66: 640x640 1 sports ball, 67: 640x640 (no detections), 68: 640x640 (no detections), 69: 640x640 (no detections), 70: 640x640 1 sports ball, 1 toothbrush, 71: 640x640 (no detections), 72: 640x640 1 sports ball, 102.6ms\n",
      "Speed: 2.2ms preprocess, 1.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mpreds/baseline\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (SSLError), entering retry loop.\n",
      "wandb: Network error (SSLError), entering retry loop.\n",
      "wandb: Network error (SSLError), entering retry loop.\n",
      "wandb: Network error (SSLError), entering retry loop.\n",
      "wandb: Network error (SSLError), entering retry loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fiery-disco-34</strong> at: <a href='https://wandb.ai/matt24/object-detection-yolov8/runs/pnxx1nod' target=\"_blank\">https://wandb.ai/matt24/object-detection-yolov8/runs/pnxx1nod</a><br/> View job at <a href='https://wandb.ai/matt24/object-detection-yolov8/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk0ODQyMTk4/version_details/v19' target=\"_blank\">https://wandb.ai/matt24/object-detection-yolov8/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk0ODQyMTk4/version_details/v19</a><br/>Synced 6 W&B file(s), 73 media file(s), 5 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230904_140501-pnxx1nod/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yolo_base = YOLO(checkpoint)\n",
    "\n",
    "for cb_name, fb_fn in callbacks.items():\n",
    "    yolo_base.add_callback(cb_name, fb_fn)\n",
    "\n",
    "preds = yolo_base.predict(val_img_paths, save=True, project=\"preds\", name=\"baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_finetuned = YOLO(checkpoint)  # load a pretrained model (recommended for training)\n",
    "for cb_name, cb_fn in callbacks.items():\n",
    "    yolo_finetuned.add_callback(cb_name, cb_fn)\n",
    "\n",
    "dataset_yaml_path = \"/workspace/object-detection-balloons/bloodcells.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.170 üöÄ Python-3.10.6 torch-2.0.1+cu118 CUDA:0 (Tesla V100-FHHL-16GB, 16151MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/workspace/object-detection-balloons/bloodcells.yaml, epochs=2, patience=50, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model summary: 225 layers, 3011433 parameters, 3011417 gradients\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/object-detection-balloons/wandb/run-20230904_152906-hvajoeva</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matt24/YOLOv8/runs/hvajoeva' target=\"_blank\">soft-feather-5</a></strong> to <a href='https://wandb.ai/matt24/YOLOv8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matt24/YOLOv8' target=\"_blank\">https://wandb.ai/matt24/YOLOv8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matt24/YOLOv8/runs/hvajoeva' target=\"_blank\">https://wandb.ai/matt24/YOLOv8/runs/hvajoeva</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Use the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[39m=\u001b[39m yolo_finetuned\u001b[39m.\u001b[39;49mtrain(data\u001b[39m=\u001b[39;49mdataset_yaml_path, epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, batch\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)  \u001b[39m# train the model\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py:341\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmodel\n\u001b[1;32m    340\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mhub_session \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession  \u001b[39m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    342\u001b[0m \u001b[39m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m RANK \u001b[39min\u001b[39;00m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py:195\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m         ddp_cleanup(\u001b[39mself\u001b[39m, \u001b[39mstr\u001b[39m(file))\n\u001b[1;32m    194\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_train(world_size)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py:293\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39mif\u001b[39;00m world_size \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup_ddp(world_size)\n\u001b[0;32m--> 293\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_train(world_size)\n\u001b[1;32m    295\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_time \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py:239\u001b[0m, in \u001b[0;36mBaseTrainer._setup_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mamp \u001b[39mand\u001b[39;00m RANK \u001b[39min\u001b[39;00m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m):  \u001b[39m# Single-GPU and DDP\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     callbacks_backup \u001b[39m=\u001b[39m callbacks\u001b[39m.\u001b[39mdefault_callbacks\u001b[39m.\u001b[39mcopy()  \u001b[39m# backup callbacks as check_amp() resets them\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mamp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(check_amp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel), device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    240\u001b[0m     callbacks\u001b[39m.\u001b[39mdefault_callbacks \u001b[39m=\u001b[39m callbacks_backup  \u001b[39m# restore callbacks\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39mif\u001b[39;00m RANK \u001b[39m>\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m world_size \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:  \u001b[39m# DDP\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/utils/checks.py:470\u001b[0m, in \u001b[0;36mcheck_amp\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39multralytics\u001b[39;00m \u001b[39mimport\u001b[39;00m YOLO\n\u001b[0;32m--> 470\u001b[0m     \u001b[39massert\u001b[39;00m amp_allclose(YOLO(\u001b[39m'\u001b[39;49m\u001b[39myolov8n.pt\u001b[39;49m\u001b[39m'\u001b[39;49m), im)\n\u001b[1;32m    471\u001b[0m     LOGGER\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mprefix\u001b[39m}\u001b[39;00m\u001b[39mchecks passed ‚úÖ\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    472\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/utils/checks.py:458\u001b[0m, in \u001b[0;36mcheck_amp.<locals>.amp_allclose\u001b[0;34m(m, im)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mamp_allclose\u001b[39m(m, im):\n\u001b[1;32m    457\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"All close FP32 vs AMP results.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m     a \u001b[39m=\u001b[39m m(im, device\u001b[39m=\u001b[39;49mdevice, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mboxes\u001b[39m.\u001b[39mdata  \u001b[39m# FP32 inference\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    460\u001b[0m         b \u001b[39m=\u001b[39m m(im, device\u001b[39m=\u001b[39mdevice, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mboxes\u001b[39m.\u001b[39mdata  \u001b[39m# AMP inference\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py:96\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, source\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     95\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(source, stream, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py:231\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor:\n\u001b[1;32m    230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor \u001b[39m=\u001b[39m (predictor \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmart_load(\u001b[39m'\u001b[39m\u001b[39mpredictor\u001b[39m\u001b[39m'\u001b[39m))(overrides\u001b[39m=\u001b[39margs, _callbacks\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks)\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor\u001b[39m.\u001b[39;49msetup_model(model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, verbose\u001b[39m=\u001b[39;49mis_cli)\n\u001b[1;32m    232\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# only update args if predictor is already setup\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m get_cfg(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs, args)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py:305\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[0;34m(self, model, verbose)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup_model\u001b[39m(\u001b[39mself\u001b[39m, model, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    304\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Initialize YOLO model with given parameters and set it to evaluation mode.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m AutoBackend(model \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    306\u001b[0m                              device\u001b[39m=\u001b[39;49mselect_device(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mdevice, verbose\u001b[39m=\u001b[39;49mverbose),\n\u001b[1;32m    307\u001b[0m                              dnn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mdnn,\n\u001b[1;32m    308\u001b[0m                              data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mdata,\n\u001b[1;32m    309\u001b[0m                              fp16\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mhalf,\n\u001b[1;32m    310\u001b[0m                              fuse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    311\u001b[0m                              verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m    313\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mdevice  \u001b[39m# update device\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhalf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfp16  \u001b[39m# update half\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/autobackend.py:102\u001b[0m, in \u001b[0;36mAutoBackend.__init__\u001b[0;34m(self, weights, device, dnn, data, fp16, fuse, verbose)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m nn_module:  \u001b[39m# in-memory PyTorch model\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     model \u001b[39m=\u001b[39m weights\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 102\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfuse(verbose\u001b[39m=\u001b[39;49mverbose) \u001b[39mif\u001b[39;00m fuse \u001b[39melse\u001b[39;00m model\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m'\u001b[39m\u001b[39mkpt_shape\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    104\u001b[0m         kpt_shape \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mkpt_shape  \u001b[39m# pose-only\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py:132\u001b[0m, in \u001b[0;36mBaseModel.fuse\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(m, Conv2):\n\u001b[1;32m    131\u001b[0m     m\u001b[39m.\u001b[39mfuse_convs()\n\u001b[0;32m--> 132\u001b[0m m\u001b[39m.\u001b[39mconv \u001b[39m=\u001b[39m fuse_conv_and_bn(m\u001b[39m.\u001b[39;49mconv, m\u001b[39m.\u001b[39;49mbn)  \u001b[39m# update conv\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39mdelattr\u001b[39m(m, \u001b[39m'\u001b[39m\u001b[39mbn\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# remove batchnorm\u001b[39;00m\n\u001b[1;32m    134\u001b[0m m\u001b[39m.\u001b[39mforward \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mforward_fuse  \u001b[39m# update forward\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/utils/torch_utils.py:122\u001b[0m, in \u001b[0;36mfuse_conv_and_bn\u001b[0;34m(conv, bn)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfuse_conv_and_bn\u001b[39m(conv, bn):\n\u001b[1;32m    121\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fuse Conv2d() and BatchNorm2d() layers https://tehnokv.com/posts/fusing-batchnorm-and-conv/.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     fusedconv \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mConv2d(conv\u001b[39m.\u001b[39;49min_channels,\n\u001b[1;32m    123\u001b[0m                           conv\u001b[39m.\u001b[39;49mout_channels,\n\u001b[1;32m    124\u001b[0m                           kernel_size\u001b[39m=\u001b[39;49mconv\u001b[39m.\u001b[39;49mkernel_size,\n\u001b[1;32m    125\u001b[0m                           stride\u001b[39m=\u001b[39;49mconv\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    126\u001b[0m                           padding\u001b[39m=\u001b[39;49mconv\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    127\u001b[0m                           dilation\u001b[39m=\u001b[39;49mconv\u001b[39m.\u001b[39;49mdilation,\n\u001b[1;32m    128\u001b[0m                           groups\u001b[39m=\u001b[39;49mconv\u001b[39m.\u001b[39;49mgroups,\n\u001b[1;32m    129\u001b[0m                           bias\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mto(conv\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    131\u001b[0m     \u001b[39m# Prepare filters\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     w_conv \u001b[39m=\u001b[39m conv\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mview(conv\u001b[39m.\u001b[39mout_channels, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:450\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    448\u001b[0m padding_ \u001b[39m=\u001b[39m padding \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(padding, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m _pair(padding)\n\u001b[1;32m    449\u001b[0m dilation_ \u001b[39m=\u001b[39m _pair(dilation)\n\u001b[0;32m--> 450\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    451\u001b[0m     in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n\u001b[1;32m    452\u001b[0m     \u001b[39mFalse\u001b[39;49;00m, _pair(\u001b[39m0\u001b[39;49m), groups, bias, padding_mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:144\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 144\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:150\u001b[0m, in \u001b[0;36m_ConvNd.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[39m# uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[39m# For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m    151\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use the model\n",
    "results = yolo_finetuned.train(data=dataset_yaml_path, epochs=2, batch=32)  # train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 30 rbcs, 1 wbc, 1: 640x640 1 platelets, 15 rbcs, 1 wbc, 2: 640x640 2 plateletss, 29 rbcs, 1 wbc, 3: 640x640 1 platelets, 24 rbcs, 1 wbc, 4: 640x640 2 plateletss, 21 rbcs, 1 wbc, 5: 640x640 1 platelets, 29 rbcs, 1 wbc, 6: 640x640 15 rbcs, 1 wbc, 7: 640x640 2 plateletss, 14 rbcs, 1 wbc, 8: 640x640 17 rbcs, 1 wbc, 9: 640x640 3 plateletss, 23 rbcs, 1 wbc, 10: 640x640 1 platelets, 17 rbcs, 2 wbcs, 11: 640x640 1 platelets, 16 rbcs, 1 wbc, 12: 640x640 18 rbcs, 1 wbc, 13: 640x640 1 platelets, 20 rbcs, 1 wbc, 14: 640x640 5 plateletss, 23 rbcs, 1 wbc, 15: 640x640 1 platelets, 16 rbcs, 1 wbc, 16: 640x640 1 platelets, 18 rbcs, 1 wbc, 17: 640x640 1 platelets, 27 rbcs, 1 wbc, 18: 640x640 1 platelets, 27 rbcs, 1 wbc, 19: 640x640 1 platelets, 26 rbcs, 1 wbc, 20: 640x640 22 rbcs, 1 wbc, 21: 640x640 2 plateletss, 20 rbcs, 2 wbcs, 22: 640x640 23 rbcs, 1 wbc, 23: 640x640 23 rbcs, 1 wbc, 24: 640x640 1 platelets, 20 rbcs, 1 wbc, 25: 640x640 22 rbcs, 1 wbc, 26: 640x640 3 plateletss, 25 rbcs, 1 wbc, 27: 640x640 19 rbcs, 1 wbc, 28: 640x640 1 platelets, 20 rbcs, 1 wbc, 29: 640x640 1 platelets, 12 rbcs, 1 wbc, 30: 640x640 2 plateletss, 22 rbcs, 1 wbc, 31: 640x640 2 plateletss, 23 rbcs, 1 wbc, 32: 640x640 26 rbcs, 1 wbc, 33: 640x640 21 rbcs, 1 wbc, 34: 640x640 2 plateletss, 23 rbcs, 1 wbc, 35: 640x640 2 plateletss, 19 rbcs, 1 wbc, 36: 640x640 1 platelets, 22 rbcs, 1 wbc, 37: 640x640 1 platelets, 20 rbcs, 1 wbc, 38: 640x640 2 plateletss, 27 rbcs, 1 wbc, 39: 640x640 2 plateletss, 26 rbcs, 1 wbc, 40: 640x640 1 platelets, 22 rbcs, 1 wbc, 41: 640x640 2 plateletss, 27 rbcs, 1 wbc, 42: 640x640 3 plateletss, 24 rbcs, 1 wbc, 43: 640x640 4 plateletss, 25 rbcs, 1 wbc, 44: 640x640 2 plateletss, 24 rbcs, 1 wbc, 45: 640x640 2 plateletss, 19 rbcs, 1 wbc, 46: 640x640 19 rbcs, 1 wbc, 47: 640x640 1 platelets, 28 rbcs, 1 wbc, 48: 640x640 1 platelets, 23 rbcs, 1 wbc, 49: 640x640 2 plateletss, 31 rbcs, 1 wbc, 50: 640x640 3 plateletss, 20 rbcs, 1 wbc, 51: 640x640 1 platelets, 18 rbcs, 2 wbcs, 52: 640x640 3 plateletss, 21 rbcs, 1 wbc, 53: 640x640 2 plateletss, 22 rbcs, 1 wbc, 54: 640x640 1 platelets, 23 rbcs, 1 wbc, 55: 640x640 20 rbcs, 1 wbc, 56: 640x640 1 platelets, 18 rbcs, 1 wbc, 57: 640x640 1 platelets, 20 rbcs, 1 wbc, 58: 640x640 1 platelets, 19 rbcs, 1 wbc, 59: 640x640 1 platelets, 25 rbcs, 1 wbc, 60: 640x640 2 plateletss, 22 rbcs, 1 wbc, 61: 640x640 1 platelets, 22 rbcs, 1 wbc, 62: 640x640 26 rbcs, 1 wbc, 63: 640x640 3 plateletss, 27 rbcs, 1 wbc, 64: 640x640 1 platelets, 23 rbcs, 1 wbc, 65: 640x640 23 rbcs, 1 wbc, 66: 640x640 2 plateletss, 19 rbcs, 1 wbc, 67: 640x640 17 rbcs, 1 wbc, 68: 640x640 1 platelets, 17 rbcs, 1 wbc, 69: 640x640 1 platelets, 25 rbcs, 1 wbc, 70: 640x640 2 plateletss, 24 rbcs, 1 wbc, 71: 640x640 1 platelets, 23 rbcs, 1 wbc, 72: 640x640 4 plateletss, 26 rbcs, 1 wbc, 332.5ms\n",
      "Speed: 1.3ms preprocess, 4.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mpreds/val\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /workspace/object-detection-balloons/preds/val\n",
    "preds = yolo_finetuned.predict(val_img_paths, save=True, project=\"preds\", name=\"val\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 platelets, 25 rbcs, 1 wbc, 1: 640x640 3 plateletss, 24 rbcs, 2 wbcs, 2: 640x640 1 platelets, 22 rbcs, 1 wbc, 3: 640x640 4 plateletss, 16 rbcs, 1 wbc, 4: 640x640 2 plateletss, 25 rbcs, 1 wbc, 5: 640x640 2 plateletss, 27 rbcs, 1 wbc, 6: 640x640 1 platelets, 17 rbcs, 1 wbc, 7: 640x640 20 rbcs, 1 wbc, 8: 640x640 1 platelets, 23 rbcs, 1 wbc, 9: 640x640 3 plateletss, 16 rbcs, 2 wbcs, 10: 640x640 2 plateletss, 22 rbcs, 1 wbc, 11: 640x640 1 platelets, 16 rbcs, 1 wbc, 12: 640x640 1 platelets, 27 rbcs, 1 wbc, 13: 640x640 20 rbcs, 1 wbc, 14: 640x640 2 plateletss, 25 rbcs, 1 wbc, 15: 640x640 14 rbcs, 1 wbc, 16: 640x640 22 rbcs, 1 wbc, 17: 640x640 18 rbcs, 1 wbc, 18: 640x640 2 plateletss, 18 rbcs, 1 wbc, 19: 640x640 2 plateletss, 29 rbcs, 2 wbcs, 20: 640x640 1 platelets, 19 rbcs, 1 wbc, 21: 640x640 2 plateletss, 30 rbcs, 1 wbc, 22: 640x640 3 plateletss, 18 rbcs, 1 wbc, 23: 640x640 18 rbcs, 1 wbc, 24: 640x640 21 rbcs, 1 wbc, 25: 640x640 2 plateletss, 22 rbcs, 1 wbc, 26: 640x640 1 platelets, 23 rbcs, 1 wbc, 27: 640x640 2 plateletss, 21 rbcs, 1 wbc, 28: 640x640 21 rbcs, 1 wbc, 29: 640x640 26 rbcs, 1 wbc, 30: 640x640 29 rbcs, 1 wbc, 31: 640x640 11 rbcs, 1 wbc, 32: 640x640 1 platelets, 23 rbcs, 1 wbc, 33: 640x640 3 plateletss, 27 rbcs, 1 wbc, 34: 640x640 3 plateletss, 19 rbcs, 1 wbc, 35: 640x640 1 platelets, 17 rbcs, 1 wbc, 163.9ms\n",
      "Speed: 1.3ms preprocess, 4.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mpreds/test\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /workspace/object-detection-balloons/preds/test\n",
    "preds = yolo_finetuned.predict(test_img_paths, save=True, project=\"preds\", name=\"test\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
