{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.10)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.35)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.30.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.24.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U -r requirements.txt\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "import ast\n",
    "import pybboxes as pbx\n",
    "\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import wandb\n",
    "\n",
    "import random\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"object-detection-yolov8\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"yolov8_bloodcells_finetuning.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = wandb.init()\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.yolo.utils.torch_utils import get_flops, get_num_params\n",
    "\n",
    "# try:\n",
    "#     import wandb\n",
    "\n",
    "#     assert hasattr(wandb, '__version__')\n",
    "# except (ImportError, AssertionError):\n",
    "#     wandb = None\n",
    "\n",
    "\n",
    "def on_pretrain_routine_start(trainer):\n",
    "    wandb.init(\n",
    "        project=os.getenv(\"WANDB_PROJECT\"), \n",
    "        # project=trainer.args.project or \"YOLOv8\", \n",
    "        name=trainer.args.name, \n",
    "        save_code=False,\n",
    "        config=dict(trainer.args)\n",
    "    )\n",
    "    # ) if not wandb.run else wandb.run\n",
    "\n",
    "\n",
    "def on_fit_epoch_end(trainer):\n",
    "    wandb.run.log(trainer.metrics, step=trainer.epoch + 1)\n",
    "\n",
    "    if trainer.epoch == 0:\n",
    "        model_info = {\n",
    "            \"model/parameters\": get_num_params(trainer.model),\n",
    "            \"model/GFLOPs\": round(get_flops(trainer.model), 3),\n",
    "            \"model/speed(ms)\": round(sum(trainer.validator.speed.values()), 3)\n",
    "        }\n",
    "        wandb.run.log(model_info, step=trainer.epoch + 1)\n",
    "\n",
    " \n",
    "def on_train_epoch_end(trainer):\n",
    "    wandb.run.log(trainer.label_loss_items(trainer.tloss, prefix=\"train\"), step=trainer.epoch + 1)\n",
    "    wandb.run.log(trainer.lr, step=trainer.epoch + 1)\n",
    "\n",
    "    if trainer.epoch == 0:\n",
    "        print(f\"[INFO] Logging atraining artifacts for Epoch {trainer.epoch + 1}...\")\n",
    "        # wandb.run.log({f.stem: wandb.Image(str(f)) for f in trainer.save_dir.glob('train_batch*.jpg')}, step=trainer.epoch + 1)\n",
    "        images = [wandb.Image(str(img_path), caption=img_path.stem) for img_path in Path(trainer.save_dir).glob(\"*.jpg\")]\n",
    "        wandb.run.log({f\"training artifacts\": images}, step=trainer.epoch + 1)\n",
    "\n",
    "\n",
    "def on_train_end(trainer):\n",
    "    art = wandb.Artifact(type=\"model\", name=f\"run_{wandb.run.id}_model\")\n",
    "    if trainer.best.exists():\n",
    "        art.add_file(trainer.best)\n",
    "        wandb.run.log_artifact(art)\n",
    "    \n",
    "\n",
    "\n",
    "def teardown(trainer):\n",
    "    wandb.finish()\n",
    "\n",
    "def on_predict_start(predictor):\n",
    "    wandb.init(\n",
    "        project=os.getenv(\"WANDB_PROJECT\"), \n",
    "        # name=predictor.args.name,\n",
    "        job_type=\"prediction\",\n",
    "        tags=[\"prediction\", predictor.args.name],\n",
    "        save_code=True,\n",
    "        config=dict(predictor.args)\n",
    "    )\n",
    "    # ) if not wandb.run else wandb.run\n",
    "\n",
    "def on_predict_end(predictor):\n",
    "    split = Path(predictor.data_path).parent.stem\n",
    "    images = [wandb.Image(str(img_path), caption=img_path.stem) for img_path in Path(predictor.save_dir).glob(\"*.jpg\")]\n",
    "    wandb.run.log({f\"{split} set | {predictor.args.name} | pred\": images})\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "callbacks = {\n",
    "    \"on_pretrain_routine_start\": on_pretrain_routine_start,\n",
    "    \"teardown\": teardown,\n",
    "    \"on_train_epoch_end\": on_train_epoch_end,\n",
    "    \"on_fit_epoch_end\": on_fit_epoch_end,\n",
    "    \"on_train_end\": on_train_end,\n",
    "    \"on_predict_start\": on_predict_start,\n",
    "    \"on_predict_end\": on_predict_end,\n",
    "} if wandb else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = {\n",
    "#     \"on_pretrain_routine_start\": on_pretrain_routine_start,\n",
    "#     \"on_train_epoch_end\": on_train_epoch_end,\n",
    "#     \"on_fit_epoch_end\": on_fit_epoch_end,\n",
    "#     \"on_train_end\": on_train_end,\n",
    "#     \"on_predict_start\": on_predict_start,\n",
    "#     \"on_predict_end\": on_predict_end,\n",
    "# } if wandb else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"keremberke/blood-cell-object-detection\", \"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf workspace/object-detection-balloons/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/workspace/object-detection-balloons/datasets/bloodcells\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm -rf /workspace/object-detection-balloons/datasets/bloodcells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in dataset:\n",
    "    for row in dataset[split]:\n",
    "        img_path = Path(os.path.join(dataset_dir, \"images\", split, str(row[\"image_id\"])) + \".jpg\")\n",
    "        os.makedirs(img_path.parent, exist_ok=True)\n",
    "        row[\"image\"].save(img_path)\n",
    "        for bbox, category in zip(row[\"objects\"][\"bbox\"], row[\"objects\"][\"category\"]):\n",
    "            bbox_yolo = pbx.convert_bbox(bbox, from_type=\"coco\", to_type=\"yolo\", image_size=(row[\"width\"], row[\"height\"]))\n",
    "            bbox_yolo = \" \".join([str(bb) for bb in bbox_yolo])\n",
    "            file_path = Path(os.path.join(dataset_dir, \"labels\", split, str(row[\"image_id\"])) + \".txt\")\n",
    "            os.makedirs(file_path.parent, exist_ok=True)\n",
    "            with open(file_path, \"a\") as f:\n",
    "                f.write(f\"{category} {bbox_yolo}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.173 🚀 Python-3.10.6 torch-2.0.1+cu118 CUDA:0 (NVIDIA RTX A4000, 16109MiB)\n",
      "Setup complete ✅ (12 CPUs, 62.8 GB RAM, 0.9/20.0 GB disk)\n"
     ]
    }
   ],
   "source": [
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/workspace/object-detection-balloons/datasets/bloodcells/images/train/3.jpg', '/workspace/object-detection-balloons/datasets/bloodcells/images/train/129.jpg', '/workspace/object-detection-balloons/datasets/bloodcells/images/train/153.jpg']\n",
      "['/workspace/object-detection-balloons/datasets/bloodcells/images/validation/51.jpg', '/workspace/object-detection-balloons/datasets/bloodcells/images/validation/14.jpg', '/workspace/object-detection-balloons/datasets/bloodcells/images/validation/48.jpg']\n",
      "['/workspace/object-detection-balloons/datasets/bloodcells/images/test/10.jpg', '/workspace/object-detection-balloons/datasets/bloodcells/images/test/32.jpg', '/workspace/object-detection-balloons/datasets/bloodcells/images/test/34.jpg']\n"
     ]
    }
   ],
   "source": [
    "train_img_base_path = \"/workspace/object-detection-balloons/datasets/bloodcells/images/train\"\n",
    "train_img_paths = [os.path.join(train_img_base_path, fname) for fname in os.listdir(train_img_base_path)]\n",
    "print(train_img_paths[:3])\n",
    "\n",
    "val_img_base_path = \"/workspace/object-detection-balloons/datasets/bloodcells/images/validation\"\n",
    "val_img_paths = [os.path.join(val_img_base_path, fname) for fname in os.listdir(val_img_base_path)]\n",
    "print(val_img_paths[:3])\n",
    "\n",
    "test_img_base_path = \"/workspace/object-detection-balloons/datasets/bloodcells/images/test\"\n",
    "test_img_paths = [os.path.join(test_img_base_path, fname) for fname in os.listdir(test_img_base_path)]\n",
    "print(test_img_paths[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for split in dataset:\n",
    "#     for row in dataset[split]:\n",
    "#         img_path = Path(os.path.join(dataset_dir, \"images\", split, str(row[\"image_id\"])) + \".jpg\")\n",
    "#         os.makedirs(img_path.parent, exist_ok=True)\n",
    "#         row[\"image\"].save(img_path)\n",
    "#         for bbox, category in zip(row[\"objects\"][\"bbox\"], row[\"objects\"][\"category\"]):\n",
    "#             bbox_yolo = pbx.convert_bbox(bbox, from_type=\"coco\", to_type=\"yolo\", image_size=(row[\"width\"], row[\"height\"]))\n",
    "#             bbox_yolo = \" \".join([str(bb) for bb in bbox_yolo])\n",
    "#             file_path = Path(os.path.join(dataset_dir, \"labels\", split, str(row[\"image_id\"])) + \".txt\")\n",
    "#             os.makedirs(file_path.parent, exist_ok=True)\n",
    "#             with open(file_path, \"a\") as f:\n",
    "#                 f.write(f\"{category} {bbox_yolo}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Log images on WANDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(job_type=\"dataset\")\n",
    "\n",
    "class_id_to_label = {\n",
    "    0: \"platelets\",\n",
    "    1: \"rbc\",\n",
    "    2: \"wbc\",\n",
    "}\n",
    "\n",
    "images = []\n",
    "for split in tqdm(dataset):\n",
    "    for row in dataset[split]:\n",
    "        positions = [\n",
    "            dict(zip((\"minX\", \"minY\", \"maxX\", \"maxY\"), pbx.convert_bbox(bbox, from_type=\"coco\", to_type=\"voc\", image_size=(row[\"width\"], row[\"height\"]))))\n",
    "            for bbox in row[\"objects\"][\"bbox\"]\n",
    "        ]\n",
    "\n",
    "        class_ids = row[\"objects\"][\"category\"]\n",
    "        box_captions = [class_id_to_label[id] for id in class_ids]\n",
    "\n",
    "        box_data = [dict(zip((\"position\", \"class_id\", \"box_caption\", \"domain\"), x)) for x in zip(positions, class_ids, box_captions, [\"pixel\"] * len(class_ids))]\n",
    "\n",
    "        images.append(wandb.Image(row[\"image\"], caption=str(row[\"image_id\"]), boxes={\"ground_truth\": {\"box_data\": box_data}}))\n",
    "\n",
    "    # run.finish()\n",
    "    # break\n",
    "    run.log({f\"{split}_set\": images})\n",
    "    images = []\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'yolov8n.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 toothbrush, 1: 640x640 (no detections), 2: 640x640 1 person, 3: 640x640 1 teddy bear, 4: 640x640 1 person, 5: 640x640 (no detections), 6: 640x640 2 persons, 7: 640x640 (no detections), 8: 640x640 (no detections), 9: 640x640 (no detections), 10: 640x640 1 person, 11: 640x640 1 person, 12: 640x640 2 persons, 13: 640x640 (no detections), 14: 640x640 (no detections), 15: 640x640 1 person, 16: 640x640 (no detections), 17: 640x640 (no detections), 18: 640x640 1 person, 1 sports ball, 19: 640x640 1 person, 20: 640x640 1 toothbrush, 21: 640x640 1 person, 22: 640x640 (no detections), 23: 640x640 1 toothbrush, 24: 640x640 1 person, 1 toothbrush, 25: 640x640 1 donut, 26: 640x640 2 persons, 27: 640x640 (no detections), 28: 640x640 (no detections), 29: 640x640 (no detections), 30: 640x640 2 sports balls, 1 toothbrush, 31: 640x640 1 donut, 32: 640x640 1 sports ball, 33: 640x640 1 person, 1 sports ball, 34: 640x640 (no detections), 35: 640x640 (no detections), 36: 640x640 (no detections), 37: 640x640 1 donut, 38: 640x640 1 cake, 39: 640x640 1 donut, 40: 640x640 (no detections), 41: 640x640 (no detections), 42: 640x640 1 donut, 43: 640x640 (no detections), 44: 640x640 1 toothbrush, 45: 640x640 (no detections), 46: 640x640 (no detections), 47: 640x640 (no detections), 48: 640x640 1 person, 1 sports ball, 49: 640x640 1 sports ball, 50: 640x640 2 sports balls, 51: 640x640 1 sports ball, 52: 640x640 (no detections), 53: 640x640 1 donut, 1 toothbrush, 54: 640x640 1 toothbrush, 55: 640x640 (no detections), 56: 640x640 2 toothbrushs, 57: 640x640 1 sports ball, 58: 640x640 (no detections), 59: 640x640 (no detections), 60: 640x640 1 toothbrush, 61: 640x640 2 sports balls, 62: 640x640 1 toothbrush, 63: 640x640 1 toothbrush, 64: 640x640 (no detections), 65: 640x640 (no detections), 66: 640x640 1 sports ball, 67: 640x640 (no detections), 68: 640x640 (no detections), 69: 640x640 (no detections), 70: 640x640 1 sports ball, 1 toothbrush, 71: 640x640 (no detections), 72: 640x640 1 sports ball, 171.1ms\n",
      "Speed: 3.4ms preprocess, 2.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mpreds/baseline7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "yolo_base = YOLO(checkpoint)\n",
    "\n",
    "for cb_name, fb_fn in callbacks.items():\n",
    "    yolo_base.add_callback(cb_name, fb_fn)\n",
    "\n",
    "preds = yolo_base.predict(val_img_paths, save=True, project=\"preds\", name=\"baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_finetuned = YOLO(checkpoint)  # load a pretrained model (recommended for training)\n",
    "for cb_name, cb_fn in callbacks.items():\n",
    "    yolo_finetuned.add_callback(cb_name, cb_fn)\n",
    "\n",
    "dataset_yaml_path = \"/workspace/object-detection-balloons/bloodcells.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.173 🚀 Python-3.10.6 torch-2.0.1+cu118 CUDA:0 (NVIDIA RTX A4000, 16109MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/workspace/object-detection-balloons/bloodcells.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train22\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011433 parameters, 3011417 gradients\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/object-detection-balloons/wandb/run-20230909_150048-sa9mxf0q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matt24/object-detection-yolov8/runs/sa9mxf0q' target=\"_blank\">pretty-sky-70</a></strong> to <a href='https://wandb.ai/matt24/object-detection-yolov8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matt24/object-detection-yolov8' target=\"_blank\">https://wandb.ai/matt24/object-detection-yolov8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matt24/object-detection-yolov8/runs/sa9mxf0q' target=\"_blank\">https://wandb.ai/matt24/object-detection-yolov8/runs/sa9mxf0q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /workspace/object-detection-balloons/datasets/bloodcells/labels/train.cache... 255 images, 0 backgrounds, 0 corrupt: 100%|██████████| 255/255 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /workspace/object-detection-balloons/datasets/bloodcells/labels/validation.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|██████████| 73/73 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/detect/train22/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train22\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      2.25G      1.557      3.162      1.632        210        640: 100%|██████████| 16/16 [00:03<00:00,  5.22it/s]\n",
      "WARNING:root:Images sizes do not match. This will causes images to be display incorrectly in the UI.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Logging atraining artifacts for Epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  4.98it/s]\n",
      "                   all         73        967     0.0433      0.633      0.164     0.0826\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10       2.3G      1.274      1.666      1.378        193        640: 100%|██████████| 16/16 [00:01<00:00,  8.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.11it/s]\n",
      "                   all         73        967      0.884      0.334      0.571      0.385\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      2.26G      1.185      1.295      1.323        162        640: 100%|██████████| 16/16 [00:01<00:00,  9.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  4.95it/s]\n",
      "                   all         73        967        0.8      0.631      0.597      0.419\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      2.26G      1.143      1.156      1.254        235        640: 100%|██████████| 16/16 [00:01<00:00,  9.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.10it/s]\n",
      "                   all         73        967      0.898      0.603      0.849      0.585\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      2.26G      1.138      1.106      1.249        229        640: 100%|██████████| 16/16 [00:01<00:00,  9.34it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DetectionTrainer' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43myolo_finetuned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_yaml_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# train the model\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py:341\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py:195\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py:381\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    379\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# suppress 'Detected lr_scheduler.step() before optimizer.step()'\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mon_train_epoch_end\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    384\u001b[0m \n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mema\u001b[38;5;241m.\u001b[39mupdate_attr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myaml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstride\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_weights\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py:160\u001b[0m, in \u001b[0;36mBaseTrainer.run_callbacks\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run all existing callbacks associated with a particular event.\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mget(event, []):\n\u001b[0;32m--> 160\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[62], line 45\u001b[0m, in \u001b[0;36mon_train_epoch_end\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     42\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining artifacts\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}, step\u001b[38;5;241m=\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (trainer\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(train_img_paths, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_preds\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DetectionTrainer' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# Use the model\n",
    "results = yolo_finetuned.train(data=dataset_yaml_path, epochs=10, batch=16)  # train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /workspace/object-detection-balloons/preds/val\n",
    "preds = yolo_finetuned.predict(val_img_paths, save=True, project=\"preds\", name=\"val\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /workspace/object-detection-balloons/preds/test\n",
    "preds = yolo_finetuned.predict(test_img_paths, save=True, project=\"preds\", name=\"test\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
